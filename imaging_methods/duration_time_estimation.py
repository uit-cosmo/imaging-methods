import numpy as np
from scipy import signal
from scipy.optimize import minimize, differential_evolution
import matplotlib.pyplot as plt
import warnings
from enum import Enum
import fppanalysis as fppa
from .second_order_statistics import autocorrelation, power_spectral_density


class Analytics(Enum):
    OneSided = 1
    TwoSided = 2


class SecondOrderStatistic(Enum):
    ACF = 1
    PSD = 2


analytical_expressions = {
    SecondOrderStatistic.ACF: autocorrelation,
    SecondOrderStatistic.PSD: power_spectral_density,
}

second_order_statistic = {SecondOrderStatistic.ACF: 1}


class DurationTimeEstimator:
    """
    Estimates duration time by fitting second order statistics (ACF or PSD).
    The attribute `statistic` sets the statistic to use and is an enum of type `SecondOrderStatistic`.
    The attribute `analytics` sets the analytical expression to use, either one-sided or two-sided pulse
    and is an enum of type `Analytics`.
    """

    def __init__(
        self, statistic=SecondOrderStatistic.PSD, analytics=Analytics.TwoSided
    ):
        self.statistic = statistic
        self.analytics = analytics

    def estimate_duration_time(
        self, data_series, dt, bounds=None, cutoff=None, **kwargs
    ):
        """
        Compute the power spectral density (PSD) of a data series using Welch's method,
        fit an analytical model derived from a Filtered Poisson Process (FPP) with
        two-sided exponential pulses, and optionally plot results.

        The analytical model for the PSD is:

        .. math::
            S(\omega) = \frac{2 \tau_d}{(1 + (1 - \lambda)^2 (\tau_d \omega)^2)(1 + (\lambda \tau_d \omega)^2)}

        where :math:`\omega` is the angular frequency (rad/s), :math:`\tau_d` is the duration time
        parameter, and :math:`\lambda \in [0, 1]` is the pulse function assymetry parameter.
        This model arises from the FPP model, where the signal is
        generated by a Poisson process filtered with two-sided exponential pulses, resulting
        in a Lorentzian-like PSD with two characteristic timescales.

        Parameters:
        -----------
        data_series : array-like
            Input time series data (1D).
        dt : float, optional
            Time step of the data series. If None, assumed to be 1.
        bounds : list of tuples, optional
            Specifies the lower and upper bounds for minimization. Defaults to [(dt, dt * len(data_series), (0, 1e3)]
        cutoff : float, optional
            cutoff for fitting (frequency cutoff in case of PSD, maximum time lag in case of ACF).
        kwargs : If statistic = PSD, an nperseg argument must be provided for the Welch method.

        Notes: When using PSD fit, nperseg is a key argument to obtain a good estimate. nperseg is the length of each
        segment used on the Welch algorithm and it determines the minimum frequency that can be estimated. If the data
        has been run-normalized, a good starting point is to set nperseg to the window length of the running
        normalization.

        Returns:
        --------
        taud : float
            Fitted decay time parameter.
        lam : float
            Fitted lambda parameter (in [0, 1]).
        """
        base, values = self._get_second_order_statistic(
            data_series, dt, cutoff, **kwargs
        )
        print("base: ", len(base))

        if self.analytics == Analytics.OneSided:
            if bounds is None:
                bounds = [(0.05 * dt, len(data_series) * dt)]
            x0 = [np.sqrt(len(data_series)) * dt]  # Geometrical mean of bounds
        elif self.analytics == Analytics.TwoSided:
            if bounds is None:
                bounds = [(0.05 * dt, len(data_series) * dt), (0, 1e3)]
            x0 = [np.sqrt(len(data_series)) * dt, 1]  # Geometrical mean of bounds

        use_minimize = False
        if use_minimize:
            result = minimize(
                lambda params: self._obj_fun(params, base, values),
                x0=x0,
                method="Nelder-Mead",
                bounds=bounds,
                options={"maxiter": 1000},
            )
        else:
            result = differential_evolution(
                lambda params: self._obj_fun(params, base, values),
                bounds=bounds,
                seed=42,  # For reproducibility
                popsize=100,  # Population size multiplier
                maxiter=1500,  # Maximum iterations
            )

        # Check optimization convergence
        if not result.success:
            warnings.warn(f"Optimization did not converge: {result.message}")

        if self.analytics == Analytics.OneSided:
            taud = result.x[0]
            lam = 0
        else:
            taud, lamda = result.x
            lam = 1 / (1 + lamda**2)

        if self.statistic == SecondOrderStatistic.PSD:
            nperseg = kwargs["nperseg"]
            if taud > nperseg * dt:
                warnings.warn(
                    f"Duration time larger than Welch segment, wrong estimate!"
                )

        return taud, lam

    def plot_and_fit(self, data_series, dt, ax, bounds=None, cutoff=None, **kwargs):
        if not isinstance(ax, plt.Axes):
            raise ValueError("ax must be a matplotlib.axes.Axes object")

        taud, fitted_lambda = self.estimate_duration_time(
            data_series, dt, bounds, cutoff, **kwargs
        )
        base, values = self._get_second_order_statistic(
            data_series, dt, cutoff, **kwargs
        )

        line = ax.plot(base, values)
        label = (rf"$\tau_d = {taud:.2g}\, \lambda = {fitted_lambda:.2g}$",)
        analytical = self._analytical_expression([taud, fitted_lambda], base)
        ax.plot(
            base,
            analytical,
            ls="--",
            label=label,
            color=line[0].get_color(),
        )

        y_max = max(np.max(values), np.max(analytical))
        if cutoff is not None:
            ax.vlines(-cutoff, ymin=0, ymax=y_max, color="black", linestyle="--")
            ax.vlines(cutoff, ymin=0, ymax=y_max, color="black", linestyle="--")

        if self.statistic == SecondOrderStatistic.PSD:
            ax.set_xscale("log")
            ax.set_yscale("log")
            ax.set_xlabel("Angular Frequency (rad/s)")
            ax.set_ylabel("Power Spectral Density")
        elif self.statistic == SecondOrderStatistic.ACF:
            ax.set_xlabel("Time lag $\Delta t$")
            ax.set_ylabel("Auto-correlation function")

        ax.set_xlim(min(base), max(base))
        ax.legend()

        return taud, fitted_lambda, base

    def _get_second_order_statistic(self, data_series, dt, cutoff=None, **kwargs):
        if self.statistic == SecondOrderStatistic.ACF:
            base, values = fppa.corr_fun(data_series, data_series, dt)
        elif self.statistic == SecondOrderStatistic.PSD:
            if "nperseg" in kwargs:
                nperseg = kwargs["nperseg"]
            else:
                raise TypeError("must provide nperseg for psd estimation")

            signal_normalized = (
                data_series - data_series.mean()
            ) / data_series.std()  # Normalize signal before welch
            base, values = signal.welch(signal_normalized, fs=1 / dt, nperseg=nperseg)
            base, values = base[1:], values[1:]  # Remove zero-frequency
            base = 2 * np.pi * base  # Convert to angular frequency (rad/s)

        else:
            raise NotImplementedError("Not valid statistic")

        if cutoff is not None:
            mask = np.abs(base) < cutoff
            return base[mask], values[mask]

        return base, values

    def _analytical_expression(self, params, base):
        """
        Evaluates the analytical expression for the given frequency and parameters.
        """
        if self.analytics == Analytics.OneSided:
            return analytical_expressions[self.statistic](base, params[0], 0)
        elif self.analytics == Analytics.TwoSided:
            return analytical_expressions[self.statistic](base, params[0], params[1])
        else:
            raise NotImplementedError(
                "Invalid analytics, analytics should be of type enum Analytics."
            )

    def _obj_fun(self, params, base, expected):
        # Renormalize lambda if two-sided
        if self.analytics == Analytics.TwoSided:
            untransformed_params = [params[0], 1 / (1 + params[1] ** 2)]
            analytical = self._analytical_expression(untransformed_params, base)
        else:
            analytical = self._analytical_expression(params, base)
        return np.sum((analytical - expected) ** 2)

    def _validate_input(self, data_series, dt, nperseg):
        data_series = np.asarray(data_series)
        if data_series.ndim != 1 or len(data_series) < 2:
            raise ValueError("data_series must be a 1D array with at least 2 points")
        elif dt <= 0:
            raise ValueError("dt must be positive")
        elif nperseg > len(data_series):
            raise ValueError("nperseg cannot exceed data length")
